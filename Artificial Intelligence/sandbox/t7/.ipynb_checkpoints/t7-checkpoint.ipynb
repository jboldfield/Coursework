{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 4820\n",
    "# Tutorial 7: MNIST - pixels as features\n",
    "\n",
    "### MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [MNIST database](https://en.wikipedia.org/wiki/MNIST_database) is a very famous dataset of handwritten digits and it has become a benchmark for image recognition algorithms. It consists of 70,000 images of 28 pixel by 28 pixels, each representing a handwritten digit.\n",
    "\n",
    "> TIP: Think of how many real-world applications involve recognition of handwritten digits:\n",
    "- zipcodes\n",
    "- tax declarations\n",
    "- student tests\n",
    "- ...\n",
    "\n",
    "The target variables are the ten digits from 0 to 9.\n",
    "\n",
    "Keras has it's built-in dataset for MNIST, so we will load it from there using the `load_data` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# allow multiple outputs be displayed for each cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape of the arrays of the data we received for the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape\n",
    "y_train.shape\n",
    "X_test.shape\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded data is a numpy array of order 3. It's like a 3-dimensional matrix, whose elements are identified by 3 indices.\n",
    "\n",
    "For now, it is sufficient to know that the first index (running from 0 to 59999 for X_train) locates a specific image in the dataset, while the other two indices determine a particular pixel in the picture, i.e., they run from 0 to the height and width of the image.\n",
    "\n",
    "For instance, we can select the first image in the training set and take a look at its shape by using the first index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_img = X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image is a 2D array of numbers between 0 and 255, like this:\n",
    "\n",
    "![The first number in the mnist training set is a 5](./assets/mnist_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `plt.imshow` once again to display the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(first_img, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that with the `gray` colormap, zeros are displayed as black pixels while higher numbers are displayed as lighter pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixels as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we use this whole image as an input to a classification model?\n",
    "\n",
    "So far our input datasets have always been 2D tabular sets, where table columns refer to different features and each data point occupies a row. In this case, each data point is itself a 2D table (an image), and so we need to decide how to map it to features. \n",
    "\n",
    "The simplest way to feed images to a Neural Network model is to use each pixel in the picture as an individual feature. If we do this, we will have $28 \\times 28 = 784$ independent features, each one being an integer between 0 and 255, and our dataset will become tabular once again. Each row in the tabular dataset will represent a different image, and each of the 784 columns will designate a specific pixel.\n",
    "\n",
    "The `reshape` method of a numpy array allows us to reshape any array to a new shape. For example, let's reshape the training dataset to be a tabular dataset with 60000 rows and 784 columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flat = X_train.reshape((60000, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the operation worked by printing the shape of `X_train_flat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful! Another valid syntax for `reshape` is to just specify the size of the dimensions we care about and let the method figure out the other dimension, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_flat = X_test.reshape(-1, 28*28)\n",
    "X_test_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have 2 tabular datasets like the ones we are familiar with. The features contain values between 0 and 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flat.min()\n",
    "X_train_flat.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already seen before, Neural Network models are quite sensitive to the absolute size of the input features, and hence they like features that are normalized to be somewhat between 0 and 1. \n",
    "\n",
    "We should rescale the values of our features to be between 0 and 1. Lets do it by dividing them by 255 so they will have values between 0 and 1. Notice that we need to convert the the data type to `float32` because under the hood numpy arrays are implemented in C and therefore are strongly typed.\n",
    "\n",
    "> Also, notice that we are not using `StandardScaler` that we used before because it would generate some negtive values, which we don't necessarily like here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc = X_train_flat.astype('float32') / 255.0\n",
    "X_test_sc = X_test_flat.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hotdog! We now have 2D data that we can use to train a fully connected Neural Network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding the multiclass output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are ten possible output classes, this is a Multiclass classification problem where the outputs are mutually exclusive. As we have learned before, we need to convert the labels (aka. targets) to a matrix of binary columns. In doing so, we communicate to the network that the labels are distinct and it should learn to predict the probability of an image to correspond to a specific label.\n",
    "\n",
    "In other words, our goal is to build a network with 784 inputs and 10 outputs, like the one represented in this figure so that for a given input image the network learns to indicate to which label it corresponds: \n",
    "\n",
    "![Fully connected network to solve MNIST](./assets/mnist_fc.png)\n",
    "\n",
    "Therefore we need to make sure that the shape of the label array matches the output of our network.\n",
    "\n",
    "We can convert our labels to binary arrays using the `to_categorical` utility function from `tensorflow.keras`. Let's import it before using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "\n",
    "y_train_cat.shape\n",
    "y_test_cat.shape\n",
    "\n",
    "# As you can see, this is an array of 10 numbers, zero everywhere except at position 5 \n",
    "# (remember we start counting from 0) indicating which of the 10 classes our image should be classified as.\n",
    "y_train_cat[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! We can now train a fully connected Neural Network using all what we've learned in the previous chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the model.\n",
    "\n",
    "The network configuration will be the following:\n",
    "\n",
    "- Input: 784 features\n",
    "- Layer 1: 512 nodes with Relu activation\n",
    "- Layer 2: 256 nodes with Relu activation\n",
    "- Layer 3: 128 nodes with Relu activation\n",
    "- Layer 4: 32 nodes with Relu activation\n",
    "- Output Layer: 10 nodes with Softmax activation\n",
    "\n",
    "Notice a number of things:\n",
    "\n",
    "1. We specify the size of the input in the definition of the first layer through the parameter `input_dim=784`.\n",
    "- The choice of the number of layers and the number of nodes per layer is arbitrary. Feel free to experiment with different architectures and observe:\n",
    "    - if the network performs better or worse\n",
    "    - if the training takes longer or shorter (number of epochs to reach a certain accuracy)\n",
    "- The last layer added to the stack is also the output layer. Make sure that the number of nodes in the last layer in the stack corresponds to the number of categories in your dataset\n",
    "- The last layer has a `Softmax` activation function at its output. As seen before, this is needed when the classes are mutually exclusive. In this case, an image of a digit cannot be of 2 different digits at the same time, and we need to let the model know about it.\n",
    "- Finally, the model is compiled using the `categorical_crossentropy` loss, which is the correct one for classifications with many mutually exclusive classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, input_dim=784, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))  # output\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the model summary (can you figure out how the \"Param #\" are calculated?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model has about half a million parameters, namely 570,602.\n",
    "\n",
    "Let's train it on our data for ten epochs with 128 images per batch. We will need to pass the _scaled inputs and reshaped_ outputs.\n",
    "\n",
    "Also, let's use a `validation_split` of 10%, meaning we will train the model on 90% of the training data, and evaluate its performance on the remaining 10%. This is like an internal train/test split done on the training data. It's useful when we plan to change the network and tune its architecture to maximize its ability to generalize. We will keep the actual test set for a final check once we have committed to the best architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.fit(X_train_sc, y_train_cat, batch_size=128,\n",
    "              epochs=10, verbose=1,\n",
    "              validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to be doing very well on the training data (as we can see by the `accuracy` output). \n",
    "\n",
    "Let's check if it is overfitting, i.e., if it is just memorizing the answers instead of learning general rules about the training examples.\n",
    "\n",
    "Let's plot the history of the accuracy and compare the training accuracy with the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h.history['accuracy'])\n",
    "plt.plot(h.history['val_accuracy'])\n",
    "plt.legend(['Training', 'Validation'])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already notice that while the training accuracy increases, the validation accuracy does not seem to increase as well. Let's check the performance on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(X_test_sc, y_test_cat)[1]\n",
    "\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's compare it with the performance on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = model.evaluate(X_train_sc, y_train_cat)[1]\n",
    "train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance on the test set is lower than the performance on the training set. \n",
    "\n",
    "> TIP: one question you may have is \"When is a difference between the test and train scores significant\". We can answer this question by running cross-validation to see what the standard deviation of each score is. Then we can compare their difference between the two scores with the standard deviation and see if their difference is much higher than the statistical fluctuations of each score.\n",
    "\n",
    "This difference between the train and test scores may indicate we are overfitting.\n",
    "\n",
    "This indication makes sense because the model is trained using the individual pixels as features. This implies that two images which are similar but slightly rotated or shifted have entirely different features.\n",
    "\n",
    "> To go beyond \"pixels as features\" we need to extract better features from the images and use a more specialized model such as Convolutional Neural Net (CCN). More on this subject later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__More tests__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred=model.predict(X_test_sc)\n",
    "\n",
    "y_test_class=np.argmax(y_test_cat, axis=1)\n",
    "y_pred_class=np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test_class, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test_class, y_pred_class)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(cm, cmap='Blues');\n",
    "\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy for our ANN model:', \\\n",
    "      round(accuracy_score(y_test_class, y_pred_class)*100, 2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy 98.24% is the same as what we obtained using `model.evaluate()` above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
