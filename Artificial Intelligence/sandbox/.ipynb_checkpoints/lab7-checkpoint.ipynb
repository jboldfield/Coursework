{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 4820\n",
    "# Lab 7: Padding, Stride, and Pooling in Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Gray Scale Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow multiple outputs be displayed for each cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's load an example gray scale image from [`scipy.misc`](https://docs.scipy.org/doc/scipy/reference/misc.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = misc.ascent()\n",
    "\n",
    "plt.figure(figsize=(7, 7)) # Note: (7, 7) is the dimension of the figure shown below instead of dimension of the image itself\n",
    "plt.imshow(img, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape # Note: (512, 512) is the dimension of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional layer wants an order-4 tensor as input, so first of all we need to reshape our image so that it has 4 axes and not 2. \n",
    "\n",
    "We can add one axis of length <span style=\"color:red\">1</span> for the input color channel (which is a grayscale pixel value between 0 and 255) and one axis of length <span style=\"color:blue\">1</span> for the dataset index.\n",
    "\n",
    "> Slide 4 of `CNN Intuition`: $(N, H, W, C)$=(<span style=\"color:blue\">1</span>, 512, 512, <span style=\"color:red\">1</span>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = img.reshape((1, 512, 512, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One convolutional Layer with (default) valid padding and 1 stride\n",
    "\n",
    "Convolutional layers are available in [`keras.layers.Conv2D`](https://keras.io/layers/convolutional/). Let's apply a convolutional layer to the image we loaded above and see what happens.\n",
    "\n",
    "First, let's import the `Conv2D` layer from keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by applying one large flat filter of size 11x11 pixels. This operation should result in a blurring of the image because the pixels are averaged (technically they are totalled).\n",
    "\n",
    "The syntax of[`Conv2D`](https://keras.io/layers/convolutional/) is:\n",
    "\n",
    "       Conv2D(filters, kernel_size, input_shape, ...)\n",
    "\n",
    "so we will specify <span style=\"color:purple\">1</span> for the `filter` (aka. channels in output) of and (11, 11) for the `kernel_size`. We will also initialize all the weights to one by using `kernel_initializer='ones'`. Finally we will need to pass the input shape, since this is the first layer in the network. This is the shape of a __single image__, which in this case is (512, 512, <span style=\"color:red\">1<span>).\n",
    "\n",
    "> Slide 12 of `CNN Intuition`: $(H_f, W_f, C_i, C_o)$=(11, 11, <span style=\"color:red\">1</span>, <span style=\"color:purple\">1</span>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 502, 502, 1)       122       \n",
      "=================================================================\n",
      "Total params: 122\n",
      "Trainable params: 122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(1, (11,11), kernel_initializer='ones',\n",
    "                 input_shape=(512, 512, 1)))\n",
    "model.compile('adam', 'mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q: Why `Output Shape = (None, 502, 502, 1)`?__\n",
    "\n",
    "__A:__ \n",
    "\n",
    "$$ height = \\lceil \\frac{i_v - f_v + 1}{s_v} \\rceil = \\lceil \\frac{512 - 11 + 1}{1} \\rceil =502 $$\n",
    "$$ width = \\lceil \\frac{i_h - f_h + 1}{s_h} \\rceil = \\lceil \\frac{512 - 11 + 1}{1} \\rceil =502 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One convolutional Layer with (default) valid padding and larger stride\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 168, 126, 1)       122       \n",
      "=================================================================\n",
      "Total params: 122\n",
      "Trainable params: 122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(1, (11,11), kernel_initializer='ones',\n",
    "                 strides = (3, 4),\n",
    "                 input_shape=(512, 512, 1)))\n",
    "model.compile('adam', 'mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q: Why `Output Shape = (None, 168, 126, 1)`?__\n",
    "\n",
    "__A:__ \n",
    "\n",
    "$$ height = \\lceil \\frac{i_v - f_v + 1}{s_v} \\rceil = \\lceil \\frac{512 - 11 + 1}{3} \\rceil =168 $$\n",
    "$$ width = \\lceil \\frac{i_h - f_h + 1}{s_h} \\rceil = \\lceil \\frac{512 - 11 + 1}{4} \\rceil =126 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One convolutional Layer with same padding and 1 stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 512, 512, 1)       122       \n",
      "=================================================================\n",
      "Total params: 122\n",
      "Trainable params: 122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(1, (11,11), kernel_initializer='ones',\n",
    "                 padding = 'same',\n",
    "                 strides = (1, 1),\n",
    "                 input_shape=(512, 512, 1)))\n",
    "model.compile('adam', 'mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q: Why `Output Shape = (None, 512, 512, 1)`?__\n",
    "\n",
    "__A:__ \n",
    "\n",
    "\n",
    "$$ height = \\lceil \\frac{i_v}{s_v} \\rceil = \\lceil \\frac{512}{1} \\rceil =512 $$\n",
    "$$ width = \\lceil \\frac{i_h}{s_h} \\rceil = \\lceil \\frac{512}{1} \\rceil =512 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One convolutional Layer with same padding and larger stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 86, 40, 1)         122       \n",
      "=================================================================\n",
      "Total params: 122\n",
      "Trainable params: 122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(1, (11,11), kernel_initializer='ones',\n",
    "                 padding = 'same',\n",
    "                 strides = (6, 13),\n",
    "                 input_shape=(512, 512, 1)))\n",
    "model.compile('adam', 'mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q: Why `Output Shape = (None, 86, 40, 1)`?__\n",
    "\n",
    "__A:__ \n",
    "\n",
    "$$ height = \\lceil \\frac{i_v}{s_v} \\rceil = \\lceil \\frac{512}{6} \\rceil =86 $$\n",
    "$$ width = \\lceil \\frac{i_h}{s_h} \\rceil = \\lceil \\frac{512}{13} \\rceil =40 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling layers\n",
    "\n",
    "Another layer we need to learn about is the pooling layer.\n",
    "\n",
    "Pooling reduces the size of the image by discarding some information. For example, max-pooling only preserves the maximum value in a patch and stores it in the new image, while dropping the values in the other pixels.\n",
    "\n",
    "Also, pooling patches usually do not overlap, which reduces the size of the image.\n",
    "\n",
    "If we apply pooling to the feature maps, we end up with smaller feature maps, that still retain the highest matches of our convolutional filters with the input.\n",
    "\n",
    "Average pooling is similar, only using average instead of max.\n",
    "\n",
    "These layers are available in `tensorflow.keras` as `MaxPooling2D` and `AveragePooling2D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a `MaxPooling2D` layer in a simple network (containing this single layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 86, 40, 1)         122       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 17, 8, 1)          0         \n",
      "=================================================================\n",
      "Total params: 122\n",
      "Trainable params: 122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(1, (11,11), kernel_initializer='ones',\n",
    "                 padding = 'same',\n",
    "                 strides = (6, 13),\n",
    "                 input_shape=(512, 512, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(5,5)))\n",
    "model.compile('adam', 'mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c0d287c0c8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIsAAAD8CAYAAABHPleGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADLJJREFUeJzt3W2MVmV+x/Hvj2EGRB6UIGt5qiKwCZLVNXTddtPWardRu8F90Reo29CHhKRmt9um7VazL3i7aUkfkjY2xqXdpHQ3DWJrzLpItrvWJq0VKSsLugsRxVlAKOqgZXQE/n0xt80dhPK/zjlz32eG3ycx83Rdc/3H/Dj3fc51rnMpIjDLmNbvAmzycFgszWGxNIfF0hwWS3NYLM1hsTSHxdIcFkub3svBBgcHY+bMmUV9JBWP04s+p0+fLh5jYGCguM+sWbMmfJx33nmH0dHRS/4P6GlYZs6cyc0331zUZ8aMGcXjDA0NFfcZHBwsar979+7iMebOnVvc55ZbbinuM3v27KL2jz32WKqdX4YsrVZYJN0p6UeSDkp6sKmirJ0qh0XSAPDXwF3AauBeSaubKszap86R5VPAwYh4JSLGgG8B9zRTlrVRnbAsBl7v+nq48z2bouqcDV3oVOsjd1JJ2ghshGpnNtYedY4sw8DSrq+XAEfObxQRj0TE2ohYW3p6au1SJyzPAyslXS9pCFgPPNFMWdZGlV+GIuKMpC8CO4ABYEtE7GusMmudWldwI+LbwLcbqsVazldwLa2nc0MA06eXDVnaHsrneaB8IrF0/gVgbGysuE+Vv7/KRGqGjyyW5rBYmsNiaQ6LpTksluawWJrDYmkOi6U5LJbmsFiaw2JpDoul9XQiceHChTzwwANFfbZs2VI8TpWVf6VOnjxZ3GfdunXFfaZNK//3vHfv3qL2o6OjuVqKK7HLlsNiaXUWmS2V9D1JL0naJ+nLTRZm7VPnPcsZ4A8iYrekOcALknZGxP6GarOWqXxkiYijEbG78/k7wEt4kdmU1sh7FknXAZ8EnrvAzzZK2iVp16lTp5oYzvqkdlgkzQYeA34vIj6Shu5FZlWeT2LtUfeRG4OMB2VrRGxvpiRrqzpnQwK+DrwUEX/WXEnWVnWOLJ8Bfh24XdKezn93N1SXtVCd5av/xoWfpGBTVE/nhubNm8ddd91V1GfZsmXF42zevLm4z5kzZ4ranzhxoniMKvM8VRaZlZ51nj17NtXOl/stzWGxNIfF0hwWS3NYLM1hsTSHxdIcFktzWCzNYbE0h8XSHBZL6+lE4tjYGIcPHy7qc+uttxaP8/777xf3OXDgQFH7KpOCq1atKu7z2muvFfe57bbbitofO3Ys1c5HFktzWCytiRu2ByT9l6QnmyjI2quJI8uXGV8zZFNc3bv7lwC/CjzaTDnWZnWPLH8BfAU410At1nJ1loJ8DjgeES9cot3/rUh88803qw5nLVB3Kcg6Sa8yvvPq7ZL+/vxG3SsS58+fX2M467c6C+MfioglEXEd41ve/UtEfKGxyqx1fJ3F0hq53B8R3we+38TvsvbykcXSejqROG3aNEofu1Flq7gqqwVLV/5V2fZuzpw5xX2qPHmzyiRn6vdOyG+1KclhsTSHxdIcFktzWCzNYbE0h8XSHBZLc1gszWGxNIfF0hwWS+vpROLo6Ch79uwp6rN69ericQ4ePFjcZ/ny5UXtt27dWjxGldtKq0wKDg0NTcgYPrJYmsNiaXXXDV0laZuklzvb3/1sU4VZ+9R9z/KXwHci4tckDQGzGqjJWqpyWCTNBX4B+A2AiBgDym9rs0mjzsvQcuAE8LedhfGPSrry/Ebdi8xGRkZqDGf9Vics04FbgIcj4pPA/wAPnt+oe5HZvHnzagxn/VYnLMPAcER8uInmNsbDY1NUnRWJx4DXJX288607AO/pPIXVPRv6ErC1cyb0CvCb9UuytqoVlojYA6xtqBZrOV/BtbSeTiTOmDGDFStWFPU5dOhQ8Th3312+CWzp5NvChQuLx3j00fIHZK1fv764z4svvljcJ8NHFktzWCzNYbE0h8XSHBZLc1gszWGxNIfF0hwWS3NYLM1hsTSHxdJ6OpF49uxZSu/D3bFjR/E4pZOCAIODg0XtT58+XTxGlb0br7322uI++/eX3YMmKdXORxZLq7vI7Pcl7ZP0Q0nflDSzqcKsfersN7QY+F1gbUSsAQYY3x3Epqi6L0PTgSskTWd8NeKR+iVZW9W5u/8nwGbgMHAUGImIp5sqzNqnzsvQ1cA9wPXAIuBKSR/ZnKp7ReLbb79dvVLruzovQ78MHIqIExHxAbAd+LnzG3WvSLzqqqtqDGf9Vicsh4FPS5ql8RP1O/D+zlNanfcszzG+ZHU3sLfzux5pqC5robqLzDYBmxqqxVrOV3AtradzQyMjIzz11FNFfXbt2lU8zqpVq4r7lG57t2DBguIxbrzxxuI+N9xwQ3GfZ599tqi954ascQ6LpTksluawWJrDYmkOi6U5LJbmsFiaw2JpDoulOSyW5rBYWk8nEj/44AOOHj1a1OeNN94oHmfx4sXFfWbNKtv95siR8nvTly5dWtzn4YcfLu5TulXeuXPncr+3uBK7bDkslnbJsEjaIum4pB92fW++pJ2SDnQ+Xj2xZVobZI4sfwfced73HgS+GxErge9ygX2GbOq5ZFgi4l+B8zckvgf4RufzbwCfb7gua6Gq71k+FhFHATofyx9kb5POhL/B7V6R+N577030cDaBqoblDUk/BdD5ePxiDbtXJM6c6SdyTGZVw/IEsKHz+Qbgn5spx9osc+r8TeDfgY9LGpb028DXgM9KOgB8tvO1TXGXvNwfEfde5Ed3NFyLtZyv4FpaTycSR0dHi7dkq/JUyOwKuzrjVFkpOXfu3OI+VbbXu++++4raP/nkk6l2PrJYmsNiaQ6LpTksluawWJrDYmkOi6U5LJbmsFiaw2JpDoulOSyW1vMViceOHSvqU2Uibc2aNcV9SicSly1bVjzGgQMHivtcc801xX22b99e1P6tt95KtfORxdIcFkuruiLxTyW9LOlFSY9L8t4wl4GqKxJ3Amsi4hPAj4GHGq7LWqjSisSIeDoiznS+/A9gyQTUZi3TxHuW3wIuuntD9yKzs2fPNjCc9UvdfZ2/CpwBtl6sTfcis4GBgTrDWZ9Vvs4iaQPwOeCOiIjmSrK2qhQWSXcCfwz8YkSU335vk1LVFYl/BcwBdkraI+lvJrhOa4GqKxK/PgG1WMv5Cq6l9XQi8dy5c5VWGJY6dOhQcZ+VK1cWtX/33XeLx1i0aFFxn/379xf3KT3rzD43x0cWS3NYLM1hsTSHxdIcFktzWCzNYbE0h8XSHBZLc1gszWGxNIfF0no6kbho0SIeeqhsIcCmTZuKx9mwYcOlG53n5MmTxX1KzZ49e8LHAFi+fHlR+xkzZqTa+chiaZUWmXX97A8lhaQFE1OetUnVRWZIWsr4Jg+HG67JWqrqtncAfw58BfCd/ZeJSu9ZJK0DfhIRP2i4Hmux4rMhSbOArwK/kmy/EdgIMH/+/NLhrEWqHFluAK4HfiDpVcbXOe+WdO2FGnevSOzVqaNNjOIjS0TspWu31U5g1kbEfzdYl7VQ1UVmdhmqs+3dhz+/rrFqrNV8BdfSejo3NDAwULz129DQUPE4Vd5IP/PMM0Xtb7rppuIxqvwtV1xxRXGf0oVpXmRmjXNYLM1hsTSHxdIcFktzWCzNYbE0h8XSHBZLc1gszWGxNIfF0lo/kVjlVswqE3b3339/Uftt27YVj7FixYriPqdOnSruM2/evKL22adb+shiaQ6LpVVekSjpS5J+JGmfpD+ZuBKtLSqtSJT0S8A9wCci4kZgc/OlWdtUXZH4O8DXIuL9TpvjE1CbtUzV9yyrgJ+X9JykZyT9zMUadm97NzIyUnE4a4OqYZkOXA18Gvgj4B8l6UINuxeZlZ7SWbtUDcswsD3G/SdwDvBjN6a4qmH5J+B2AEmrgCHAKxKnuEtewe2sSLwNWCBpGNgEbAG2dE6nx4AN3lRz6quzIvELDddiLecruJamXr56SDoBvHaBHy3g8n7P0++//6cj4ppLNeppWC5ahLQrItb2u45+mSx/v1+GLM1hsbS2hOWRfhfQZ5Pi72/FexabHNpyZLFJoO9hkXRn5yaqg5Ie7Hc9vSbpVUl7Je2RtKvf9fx/+voyJGkA+DHjj3UfBp4H7o2IskcXTWKT6Wmf/T6yfAo4GBGvRMQY8C3G78CzFup3WBYDr3d9Pdz53uUkgKclvdB5Gnlr9XTd0AVc6Iapy+307DMRcUTSQmCnpJc7t7K2Tr+PLMPA0q6vlwBH+lRLX0TEkc7H48DjjL80t1K/w/I8sFLS9ZKGgPXAE32uqWckXSlpzoefM755xkc2AWuLvr4MRcQZSV8EdgADwJaI2NfPmnrsY8DjnduXpwP/EBHf6W9JF+cruJbW75chm0QcFktzWCzNYbE0h8XSHBZLc1gszWGxtP8FBmM+HCAOC54AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_pred = model.predict(img_tensor)[0, :, :, 0]\n",
    "img_pred.shape\n",
    "\n",
    "plt.imshow(img_pred, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max-pooling layers are useful in tasks of object recognition, since pixels in feature maps represent the \"degree of matching\" of a filter with a receptive field, keeping the max keeps the highest matching feature. \n",
    "\n",
    "On the other hand, if we are also interested in the location of a particular match, then we shouldn't be using max-pooling, because we lose the location information in the pooling operation. \n",
    "\n",
    "Thus, for example, if we are using a Convolutional Neural Network to read the state of a video game from a frame, we need to know the exact positions of players and thus using max-pooling is not recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally `GlobalMaxPooling2D` calculates the global max in the image, so it returns a single value for the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 86, 40, 1)         122       \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 122\n",
      "Trainable params: 122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(1, (11,11), kernel_initializer='ones',\n",
    "                 padding = 'same',\n",
    "                 strides = (6, 13),\n",
    "                 input_shape=(512, 512, 1)))\n",
    "model.add(GlobalMaxPooling2D())\n",
    "model.compile('adam', 'mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_pred = model.predict(img_tensor)\n",
    "img_pred.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
