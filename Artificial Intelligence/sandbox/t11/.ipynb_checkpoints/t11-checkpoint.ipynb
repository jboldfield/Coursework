{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 4820\n",
    "# Tutorial 11: Time Series and Recurrent Neural Networks (RNN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Time series forecasting\n",
    "\n",
    "A time series can have arbitrary length, and it usually comes with a timestamp, indicating the absolute time of each sample.\n",
    "\n",
    "Let's load a dataset that contains hourly electricity demands for different parts of Canada and it runs from May 2003 to December 2016. We will see how recurrent networks can help in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow multiple outputs be displayed for each cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Jupyter Notebook\n",
    "fname = './data/ZonalDemands_2003-2016.csv.bz2'\n",
    "\n",
    "# Google colab : Importing the dataset\n",
    "# from google.colab import files\n",
    "# import io\n",
    "\n",
    "# fname = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Total Ontario</th>\n",
       "      <th>Northwest</th>\n",
       "      <th>Northeast</th>\n",
       "      <th>Ottawa</th>\n",
       "      <th>East</th>\n",
       "      <th>Toronto</th>\n",
       "      <th>Essa</th>\n",
       "      <th>Bruce</th>\n",
       "      <th>Southwest</th>\n",
       "      <th>Niagara</th>\n",
       "      <th>West</th>\n",
       "      <th>Tot Zones</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-May-03</td>\n",
       "      <td>1</td>\n",
       "      <td>13702</td>\n",
       "      <td>809</td>\n",
       "      <td>1284</td>\n",
       "      <td>965</td>\n",
       "      <td>765</td>\n",
       "      <td>4422</td>\n",
       "      <td>622</td>\n",
       "      <td>41</td>\n",
       "      <td>2729</td>\n",
       "      <td>617</td>\n",
       "      <td>1611</td>\n",
       "      <td>13865</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-May-03</td>\n",
       "      <td>2</td>\n",
       "      <td>13578</td>\n",
       "      <td>825</td>\n",
       "      <td>1283</td>\n",
       "      <td>923</td>\n",
       "      <td>752</td>\n",
       "      <td>4340</td>\n",
       "      <td>602</td>\n",
       "      <td>43</td>\n",
       "      <td>2731</td>\n",
       "      <td>615</td>\n",
       "      <td>1564</td>\n",
       "      <td>13678</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-May-03</td>\n",
       "      <td>3</td>\n",
       "      <td>13411</td>\n",
       "      <td>834</td>\n",
       "      <td>1277</td>\n",
       "      <td>910</td>\n",
       "      <td>751</td>\n",
       "      <td>4281</td>\n",
       "      <td>591</td>\n",
       "      <td>45</td>\n",
       "      <td>2696</td>\n",
       "      <td>596</td>\n",
       "      <td>1553</td>\n",
       "      <td>13534</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Hour  Total Ontario  Northwest  Northeast  Ottawa  East  \\\n",
       "0  01-May-03     1          13702        809       1284     965   765   \n",
       "1  01-May-03     2          13578        825       1283     923   752   \n",
       "2  01-May-03     3          13411        834       1277     910   751   \n",
       "\n",
       "   Toronto  Essa  Bruce  Southwest  Niagara  West  Tot Zones diff  \n",
       "0     4422   622     41       2729      617  1611      13865  163  \n",
       "1     4340   602     43       2731      615  1564      13678  100  \n",
       "2     4281   591     45       2696      596  1553      13534  123  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Total Ontario</th>\n",
       "      <th>Northwest</th>\n",
       "      <th>Northeast</th>\n",
       "      <th>Ottawa</th>\n",
       "      <th>East</th>\n",
       "      <th>Toronto</th>\n",
       "      <th>Essa</th>\n",
       "      <th>Bruce</th>\n",
       "      <th>Southwest</th>\n",
       "      <th>Niagara</th>\n",
       "      <th>West</th>\n",
       "      <th>Tot Zones</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119853</th>\n",
       "      <td>2016/12/31</td>\n",
       "      <td>22</td>\n",
       "      <td>15195</td>\n",
       "      <td>495</td>\n",
       "      <td>1476</td>\n",
       "      <td>1051</td>\n",
       "      <td>1203</td>\n",
       "      <td>5665</td>\n",
       "      <td>1045</td>\n",
       "      <td>72</td>\n",
       "      <td>2986</td>\n",
       "      <td>465</td>\n",
       "      <td>1334</td>\n",
       "      <td>15790</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119854</th>\n",
       "      <td>2016/12/31</td>\n",
       "      <td>23</td>\n",
       "      <td>14758</td>\n",
       "      <td>495</td>\n",
       "      <td>1476</td>\n",
       "      <td>1051</td>\n",
       "      <td>1203</td>\n",
       "      <td>5665</td>\n",
       "      <td>1045</td>\n",
       "      <td>72</td>\n",
       "      <td>2986</td>\n",
       "      <td>465</td>\n",
       "      <td>1334</td>\n",
       "      <td>15790</td>\n",
       "      <td>1,032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119855</th>\n",
       "      <td>2016/12/31</td>\n",
       "      <td>24</td>\n",
       "      <td>14153</td>\n",
       "      <td>495</td>\n",
       "      <td>1476</td>\n",
       "      <td>1051</td>\n",
       "      <td>1203</td>\n",
       "      <td>5665</td>\n",
       "      <td>1045</td>\n",
       "      <td>72</td>\n",
       "      <td>2986</td>\n",
       "      <td>465</td>\n",
       "      <td>1334</td>\n",
       "      <td>15790</td>\n",
       "      <td>1,637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date  Hour  Total Ontario  Northwest  Northeast  Ottawa  East  \\\n",
       "119853  2016/12/31    22          15195        495       1476    1051  1203   \n",
       "119854  2016/12/31    23          14758        495       1476    1051  1203   \n",
       "119855  2016/12/31    24          14153        495       1476    1051  1203   \n",
       "\n",
       "        Toronto  Essa  Bruce  Southwest  Niagara  West  Tot Zones   diff  \n",
       "119853     5665  1045     72       2986      465  1334      15790    595  \n",
       "119854     5665  1045     72       2986      465  1334      15790  1,032  \n",
       "119855     5665  1045     72       2986      465  1334      15790  1,637  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(fname, compression='bz2',\n",
    "                 engine='python')\n",
    "\n",
    "\n",
    "# Google colab : Read the dataset\n",
    "# df = pd.read_csv(\"ZonalDemands_2003-2016.csv.bz2\", compression='bz2',\n",
    "#                 engine='python')\n",
    "\n",
    "df.head(3)\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Generate the `DatetimeIndex` for each column\n",
    "Let's create a `pd.DatetimeIndex` using the Date and Hour columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_date_hour(row):\n",
    "    date = pd.to_datetime(row['Date'])\n",
    "    hour = pd.Timedelta(\"%d hours\" % row['Hour'])\n",
    "    return date + hour\n",
    "\n",
    "idx = df.apply(combine_date_hour, axis=1)\n",
    "idx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(idx)\n",
    "\n",
    "df.head(3)\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total Ontario'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Split the data into training and testing datasets\n",
    "\n",
    "The time series seems quite regular, which looks promising for forecasting. Let's split the data in time on January 1st, 2014. We will use data before that date as training data and data after that date as test data.\n",
    "\n",
    "In the rest of this tutorial, we will be working on the electricity demands for `Total Ontario` only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = pd.Timestamp('01-01-2014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.loc[:split_date, ['Total Ontario']].copy()\n",
    "test = df.loc[split_date:, ['Total Ontario']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)\n",
    "train.tail(3)\n",
    "\n",
    "test.head(3)\n",
    "test.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(train, label='y_test')\n",
    "plt.plot(test, label='y_pred')\n",
    "plt.legend()\n",
    "plt.title(\"Energy Consumption in Ontario 2003 - 2017\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already seen so far, the Neural Network models are quite sensitive to the absolute size of the input features. Passing features with very large or minimal values will not help our model converge to a solution. Hence, we should rescale the data before anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 10000\n",
    "scale = 5000\n",
    "\n",
    "train_sc = (train - offset) / scale\n",
    "test_sc = (test - offset) / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sc[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(train_sc, label='y_test')\n",
    "plt.plot(test_sc, label='y_pred')\n",
    "plt.legend()\n",
    "plt.title(\"Energy Consumption Scaled Data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Creating the targets\n",
    "\n",
    "We are finally ready to build a predictive model. Our target is going to be the value of the energy demand at a certain point in time and our first model will try to predict such value from the value in the preceding hour. Our model will thus only have one feature, and the labels will come from the same data, __shifted in time by one hour__.\n",
    "\n",
    "There is a neat trick to generate both labels and features from the same `train_sc` timeseries. This is achieved by taking every single point in the timeseries except the last one as features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_sc[:-1].values\n",
    "y_train = train_sc[1:].values\n",
    "\n",
    "\n",
    "X_test = test_sc[:-1].values\n",
    "y_test = test_sc[1:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see below, the first value in `y_train` corresponds to the second value in `X_train`, meaning that we will be using the first value `X_train` to predict the next value and so on. Now we have our training data as well as testing data mapped out. Let's move on to model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:5]\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fully-Connected Neural Networks Model\n",
    "\n",
    "Let's train a fully connected network to predict and see that it is not able to predict the next value from the previous one.\n",
    "\n",
    "The network will have a single input (the previous hour value) and a single output.\n",
    "\n",
    "Since we want to predict a continuous variable, the output of the network does not need an activation function, and we will use the mean_squared_error as loss function, which is a standard error metric in regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clear the backend of any held memory first before building a new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(24, input_dim=1, activation='relu'),\n",
    "    Dense(12, activation='relu'),\n",
    "    Dense(6, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, before fitting the built Neural Networks, we load the `EarlyStopping` callback, to halt the training if it is not improving.\n",
    "\n",
    ">TIP: a [callback](https://keras.io/callbacks/) is a set of functions to be applied at each epoch during the training. You can pass a list of callbacks to the `.fit()` method, and in this specific case we use the `EarlyStopping` callback to stop the training if there is no progress. According to the [documentation](https://keras.io/callbacks/), `monitor` defines the quantity to be monitored (the `mean_squared_error` in this case) and `patience` defines the number of epochs with no improvement after which it will stop the training.\n",
    "\n",
    "In particular, we will set the `EarlyStopping` callback to monitor the value of the loss and stop the training loop with a `patience=3` if that does not improve. Without this callback, the training will plateau on a fixed loss without improving, and the training will not stop by itself (go ahead and try to confirm that!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='loss',\n",
    "                           patience=3,\n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=200,\n",
    "          batch_size=512, verbose=0,\n",
    "          callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model stopped improving quite quickly. Feel free to experiment with other architectures and other activation functions. Let's see how our model is doing. We can generate the predictions on the test set by running `model.predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(y_test, label='y_test')\n",
    "plt.plot(y_pred, label='y_pred')\n",
    "plt.legend()\n",
    "plt.title(\"True VS Pred Test set, Fully Connected\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They seem to overlap pretty well. Is it so? Let's zoom in and watch more closely. We will do this by using the `plt.xlim` function that sets the boundaries of the horizontal axis in a plot. Feel free to choose other values to inspect other regions of the plot. Also, notice that we lost the date labels when we created the data, but this is not a problem: we can always bring them back from the original series if we need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(y_test, label='y_test')\n",
    "plt.plot(y_pred, label='y_pred')\n",
    "plt.legend()\n",
    "plt.xlim(1200,1300)\n",
    "plt.title(\"Zoom True VS Pred Test set, Fully Connected\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a good model? At first glance, we may be tempted to say it is.\n",
    "\n",
    "__However, the model is not good at all!__\n",
    "\n",
    "Why do we say that the model is not good at all? If you scrutinize the graph, you will realize that the network has just learned to repeat the same value it receives in input!\n",
    "\n",
    "This is not forecasting at all. In other words, the model has no real predictive power. It behaves like a parrot that repeats yesterday's value for today. In this particular case, since the curve is varying smoothly, the differences between one day and the next are small. However, the model is not anticipating any future value, and so it would be quite useless for forecasting. One easy way to see this is to measure the correlation between the predicted values and the correct labels, and then repeat the measure of  __<span style=\"color:red\">correlation with labels shifted in time</span>__.\n",
    "\n",
    "__If the model was good at forecasting we expect the highest correlation to happen for a zero shift, with decreasing correlation when we shift the labels either forward or backward in time__. Let's plug our test data into Pandas Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_s = pd.Series(y_test.ravel())\n",
    "y_pred_s = pd.Series(y_pred.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for shift in range(-5, 5):\n",
    "    y_pred_shifted = y_pred_s.shift(shift)\n",
    "    corr = y_test_s.corr(y_pred_shifted)\n",
    "    print(\"Shift: {:2}, Corr: {:0.2}\".format(shift, corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the highest correlation for this model is found for a shift of -1, which validates our previous interpretation. __<span style=\"color:red\">The model is simply copying the input value to the output.<span>__\n",
    "\n",
    "This behavior is not surprising. After all, the only feature our model knew was the value of the time series in the previous period, so it makes sense that the best it could do was to learn to repeat it as a prediction for what would come next.\n",
    "\n",
    "Let's see if a recurrent network improves the situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Vanilla Recurrent Neural Networks (RNN) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Reshape the data\n",
    "\n",
    "The [documentation](https://keras.io/layers/recurrent/) for recurrent layers reads:\n",
    "\n",
    "    Input shape\n",
    "\n",
    "    3D tensor with shape (batch_size, timesteps, input_dim).\n",
    "\n",
    "however, so far we have used only a tensor of second order for our data. Let's think about how to reshape our data because there's more than one way. Our input data right now has a shape of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it's like a matrix with a single column. We want to add a dimension to the tensor so that the data is a tensor of order three. There are many ways of doing this, the simplest way is to reshape the tensor using the `.reshape` method. It's as if we were breaking our timeseries into a dataset of adjacent and disjoint windows and then pile all of them one on top of the other.\n",
    "\n",
    "Let's define the window length to be 128 hours (a little over five days):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_len = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a helper function that reshapes the data into a tensor of order three. Notice that this function will pre-pad our sequence with zeros since we need to make sure that the total length is divisible by `win_len`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_tensor(x, l):\n",
    "    orig_len = x.shape[0]\n",
    "    n, r = divmod(orig_len, l)\n",
    "    max_len = l * (n + 1)\n",
    "    offset = max_len - orig_len\n",
    "    new_array = np.zeros(max_len)\n",
    "    new_array[offset:] = x.ravel()\n",
    "    return new_array.reshape(n + 1, l, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now convert our Train and Test sets by calling the function above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = reshape_tensor(X_train, win_len)\n",
    "X_test_t = reshape_tensor(X_test, win_len)\n",
    "\n",
    "y_train_t = reshape_tensor(y_train, win_len)\n",
    "y_test_t = reshape_tensor(y_test, win_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![reshape.png](./assets/reshape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Model construction\n",
    "\n",
    "First of all, let's clear the backend of any held memory first before building a new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(6,\n",
    "                    batch_input_shape=(1, win_len, 1),\n",
    "                    kernel_initializer='ones',\n",
    "                    return_sequences=True,\n",
    "                    stateful=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.0005),\n",
    "              loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the data. Since we are maintaining states between point, we shall pass the data in order using the `shuffle=False` flag and `batch_size=1`. Also, we run the training for just three epochs. In our experiments this should be sufficient to get decent results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_t, y_train_t,\n",
    "          epochs=3,\n",
    "          batch_size=1,\n",
    "          verbose=1,\n",
    "          shuffle=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a small part of our predictive model to compare train and test. We will use the Numpy `.ravel()` method to flatten the tensors back into a unidimensional sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_t, batch_size=1)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(y_test_t.ravel(), label='y_test')\n",
    "plt.plot(y_pred.ravel(), label='y_pred')\n",
    "plt.legend()\n",
    "plt.xlim(1200,1300)\n",
    "plt.title(\"Zoom True VS Pred Test set, SimpleRNN\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While writing the tutorial, I noticed that sometimes the model may converge to different solutions at each training run. \n",
    "1. Most often we get a graph that looks a little noisy but it is close to the actual data in the sharp decay phases, meaning some forecasting power is achieved:\n",
    "![](./assets/simpleRNN_1.png)\n",
    "2. Sometimes we would get a graph that looks very similar to the Fully Connected result, with no predictivity at all:\n",
    "![](./assets/simpleRNN.png)\n",
    "3. Sometimes the network will get stuck and give nonsense results like this one:\n",
    "![](./assets/simpleRNN_2.png)\n",
    "\n",
    "Feel free to change the number of layers, nodes, optimizer and learning rate to see if you can get better results. You will notice that this model is very prone to diverging away from a small value of the loss, which is not ideal at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also repeat the correlation with time-shift measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_s = pd.Series(y_test_t.ravel())\n",
    "y_pred_s = pd.Series(y_pred.ravel())\n",
    "\n",
    "for shift in range(-5, 5):\n",
    "    y_pred_shifted = y_pred_s.shift(shift)\n",
    "    corr = y_test_s.corr(y_pred_shifted)\n",
    "    print(\"Shift: {:2}, Corr: {:0.2}\".format(shift, corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all this model seems a little unstable. The problem probably lies with the fact that the `SimpleRNN` actually has a short memory and cannot learn long-term patterns. More importantanly, it suffers from the __\"Vanishing Gradient\"__ problem as described in the \"5. RNN Intuition\" slides. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Long Short Term Memory (LSTM) Neural Networks Model\n",
    "\n",
    "In this section, we will replace `SimpleRNN` with `LSTM` when building our model. \n",
    "\n",
    "Additionally, we will  improve the model in the following three ways:\n",
    "\n",
    "- (In section 4.1) Data samples should overlap for better use of the training data\n",
    "    - Consequently, (In section 4.2), no need to use the `stateful=True` option for the LSTM model\n",
    "- (In section 4.2) Each LSTM neuron only produces one output (the last one in the time sequence).\n",
    "    - No need to use the `return_sequences=True` option for the LSTM model\n",
    "    - No need to use the `TimeDistributed` wrapper\n",
    "- (In section 4.2) Train our model using a batch of size 256 instead of 1 for faster training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Rolling windows\n",
    "\n",
    "![Rolling windows](./assets/rolling_windows.png)\n",
    "\n",
    "Let's start by defining a smaller window size. We'll take a window of 24 periods, i.e., the data from the previous day. You can always adjust this later on if you wish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_len = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a helper function `create_lagged_Xy_win` to create the lagged data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_Xy_win(data, start_lag=1,\n",
    "                         window_len=1):\n",
    "    X = data.shift(start_lag + window_len - 1).copy()\n",
    "    X.columns = ['T_{}'.format(start_lag + window_len - 1)]\n",
    "\n",
    "    if window_len > 1:\n",
    "        for s in range(window_len, 0, -1):\n",
    "            col_ = 'T_{}'.format(start_lag + s - 1)\n",
    "            X[col_] = data.shift(start_lag + s - 1)\n",
    "\n",
    "    X = X.dropna()\n",
    "    idx = X.index\n",
    "    y = data.loc[idx]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the function on the train and test data. We will use `start_lag=1` so that we can compare the results with our previous results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lag=1\n",
    "\n",
    "X_train, y_train = create_lagged_Xy_win(train_sc,\n",
    "                                        start_lag,\n",
    "                                        window_len)\n",
    "\n",
    "X_test, y_test = create_lagged_Xy_win(test_sc,\n",
    "                                      start_lag,\n",
    "                                      window_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like described in Section 3.1 above, to feed this data to a recurrent model, we need to reshape as a tensor of order with the shape ` (batch_size, timesteps, input_dim)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = X_train.values.reshape(-1, window_len, 1)\n",
    "X_test_t = X_test.values.reshape(-1, window_len, 1)\n",
    "\n",
    "y_train_t = y_train.values\n",
    "y_test_t = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the LSTM model:\n",
    "\n",
    "- Six neurons \n",
    "- `input_shape=(window_len, 1)`. The batch size will be inferred when fitting the model.\n",
    "- No need to use the `stateful=True` option because some history is already present in the input data and the windows are overlapping (see Section 4.1 above).\n",
    "- No need to use the `return_sequences=True` option for the model\n",
    "- No need to use the `TimeDistributed` wrapper for the `Dense` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(6, input_shape=(window_len, 1),\n",
    "               kernel_initializer='ones'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=Adam(lr=0.0005) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the modle:\n",
    "\n",
    "- 10 epochs\n",
    "- `batch_size=256`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_t, y_train_t,\n",
    "          epochs=10,\n",
    "          batch_size=256,\n",
    "          verbose=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Model evaluation\n",
    "\n",
    "Let's plot a small part of our predictive model to compare train and test. __No need to use the `Numpy .ravel()` method though__:\n",
    "\n",
    "Here is a sample of the plot that is expected:\n",
    "\n",
    "![LSTM_1](./assets/LSTM_1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_t, batch_size=256)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(y_test_t, label='y_test')\n",
    "plt.plot(y_pred, label='y_pred')\n",
    "plt.legend()\n",
    "plt.xlim(1200,1300)\n",
    "plt.title(\"Zoom True VS Pred Test set, LSTM with Windows\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also repeat the correlation with time-shift measure.\n",
    "\n",
    "__Ensure the highest correlation to happen for a zero shift, with decreasing correlation when we shift the labels either forward or backward in time.__ \n",
    "\n",
    "Here is a sample of the result that is expected:\n",
    "\n",
    "![LSTM_2](./assets/LSTM_2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_s = pd.Series(y_test_t.ravel())\n",
    "y_pred_s = pd.Series(y_pred.ravel())\n",
    "\n",
    "for shift in range(-5, 5):\n",
    "    y_pred_shifted = y_pred_s.shift(shift)\n",
    "    corr = y_test_s.corr(y_pred_shifted)\n",
    "    print(\"Shift: {:2}, Corr: {:0.2}\".format(shift, corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model trained __considerably faster and more consistent__ than the simpleRNN-based model, and its predictions should look better than the previous models. First of all the model seems to have learned the temporal patter much better than the other models: it's not simply repeating the input like a parrot, it's genuinely trying to predict the future. Also, the curves look quite close to one another, which is a great sign!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
